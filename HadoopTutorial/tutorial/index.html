<!-- DOCTYPE -->
<!-- CS 496 How-To: A MapReduce / Hadoop Tutorial
Site design help from:
https://designmodo.com/bootstrap-4-tutorial/
https://blackrockdigital.github.io/startbootstrap-scrolling-nav/ -->
<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="CS 496 How-To: A MapReduce & Hadoop Tutorial">
    <meta name="author" content="Kendra Swafford">

    <title>CS496 How-To</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/scrolling-nav.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<!-- The #page-top ID is part of the scrolling feature - the data-spy and data-target are part of the built-in Bootstrap scrollspy function -->

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top" role="navigation">
        <div class="container">
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top">Getting Started With MapReduce and Hadoop</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-ex1-collapse">
                <ul class="nav navbar-nav">
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                    <li class="hidden">
                        <a class="page-scroll" href="#page-top"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#intro1">Overview</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#setup1">Setting Up</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#job1">Running a Job</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#nextsteps">Next Steps</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#resources">Resources</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Intro Section -->
    <section id="intro0" class="intro-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h2>MapReduce and Hadoop How-To</h2>
                    <p>
                        This site is intended to be an introduction to <a href="http://hadoop.apache.org/">Hadoop</a> with a brief overview of big data and the MapReduce model,
                        followed by a step-by-step for setting up a multinode Hadoop cluster with <a href="https://aws.amazon.com/ec2/">AWS EC2</a> and running a demo MapReduce job.
                        This how-to is intended to be for those with minimal programming experience. Some Java and operating systems basics will be helpful, but aren't necessary.
                    </p>
                    <img src="img/HadoopAndBigData.jpg" width="500" />

                    <h3>Why this how-to?</h3>
                    <p>
                        While there are tutorials for setting up an AWS EC2 virtual machine, and there are tutorials for setting up single and multinode Hadoop clusters,
                        and there are tutorials for running MapReduce jobs on a single-node cluster, at the time of writing this how-to, I haven't yet found a coherent start-to-finish process for beginners.
                        There are classes that 1) cost money and 2) assume you're already working as a data scientist;
                        in contrast, there are free tutorials that will get you set up quickly, but don't provide background on the <em>why</em> behind what you're doing.
                        If you are looking for a quick get-up-and-running without the details, consider the walkthrough for setting up Hadoop on Amazon Elastic MapReduce <a href="https://aws.amazon.com/emr/details/hadoop/">here</a>.
                    </p>
                    <p>
                        Otherwise, let's get started!
                    </p>
                    <a class="btn btn-default page-scroll" href="#intro1">What's MapReduce?</a>
                </div>
            </div>
        </div>
    </section>

    <!-- Intro: MapReduce Section -->
    <section id="intro1" class="intro-section2">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h3>MapReduce simplified (sort of)</h3>
                    <p>
                        <strong>MapReduce</strong> is a programming model that decomposes data sets into subsets and then operates on those subsets in a <em>parallel</em> manner. This model operates by using a <strong>map function</strong> to filter and sort key-value pairs to yield a new set of key-value pairs, which are then merged in a <strong>reduce function</strong> to yield a final set of key-value pairs.
                    </p>
                    <img src="img/dataflow.jpg" />
                    <h3>When would you want to use MapReduce?</h3>
                    <p>
                        When you have lots of data - like, Big Data - like, <em>quintillions of bytes</em> of data that you can process in parallel across multiple machines, MapReduce is a good choice.
                        Likewise, if you've got data that fits on a single machine's main memory, or a process that's pretty quick, or a process that is iterative, such as machine learning, MapReduce might not be the right choice.
                    </p>
                    <img src="img/Visualization.png" width="300" /> <br />
                    <a class="btn btn-default page-scroll" href="#example1">Let's see an example!</a>
                </div>
            </div>
        </div>
    </section>

    <!-- MapReduceExample1-->
    <section id="example1" class="intro-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h3>MapReduce Wikipedia Example: How Many Friends?</h3>
                    <p>
                        Say we've collected some data on around one billion people. Specifically, we know each person's <strong>age</strong> and <strong>number of friends</strong>; what we'd like to learn from this data is how aging affects the average number of friends a person has.
                        So the <em>processing</em> algorithm we're interested in here might look loosely like this:
                    </p>
                    <script src="https://gist.github.com/swaffork/be4e0ddbd60a7df09c21fe59fc29197e.js"></script>
                    <p>
                        Now remember, we're looking at <em>1 billion</em> records, and we've got a task (taking the average number of friends) that can be parallelized - that is, finding the average number of friends that 8 year olds have can be performed at the same time as finding the average number of friends that 72 year olds have.
                        This means that, by using the MapReduce model, we can split our data across nodes (the <strong>map</strong> step), process the data subsets in parallel, and then combine the results (the <strong>reduce</strong> step). How efficient!
                    </p>
                    <a class="btn btn-default page-scroll" href="#example2">Neat! So what would that look like?</a>
                </div>
            </div>
        </div>
    </section>

    <!-- MapReduceExample2-->
    <section id="example2" class="intro-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <p>So first, we'll divide this set of one billion people into subsets of around one million people and have each datanode map each person's age and number of friends into records (which is a bit like tallying, where age is the key and number of friends is the value):</p>
                    <script src="https://gist.github.com/swaffork/c69afc4af8dd6f59dcf4489f34e1c68c.js"></script>
                    <p>Now that we have our data <strong>mapped</strong> to the (key, value) pairs we're interested in, we can give each datanode a subpopulation on which to <strong>reduce</strong> the subpopulation by the average number of friends:</p>
                    <script src="https://gist.github.com/swaffork/c5c2526ad7a70d49c97354cf2d54884c.js"></script>
                    <p>
                        And now we have the data we wanted!
                        We've turned an unsorted population dataset of one billion people into a tidy number-of-friends-by-age collection.
                    </p>
                    <p>
                        Now that we've got an understanding of the MapReduce model...
                    </p>
                    <a class="btn btn-default page-scroll" href="#intro2">On to Hadoop!</a>
                </div>
            </div>
        </div>
    </section>

    <!-- Intro: Hadoop Section -->
    <section id="intro2" class="intro-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h1>What's Hadoop?</h1>
                    <p>
                        <strong>Hadoop</strong> is an open-source framework by Apache for distributed storage and processing of big-data sets.
                    </p>
                    <img src="img/elephant.PNG" width="300" />
                    <p>
                        Hadoop has three main pieces: storage, processing, and resource management.
                        The <strong>Hadoop Distributed File System (HDFS)</strong> comprises the storage part.
                        This system splits files into pieces and stores them redudantly on relatively cheap machines (known as <strong>DataNodes</strong>); access to these datanodes is controlled by one <strong>NameNode</strong> which hosts the file system index.
                        Data is replicated across multiple datanodes for relaibility. By indexing and tracking data across a cluster of servers, it enables the processing of lots of data. Like, <em>lots</em>.
                    </p>
                    <p>
                        The processing part of Hadoop is the <strong>MapReduce Engine</strong>.
                        The MapReduce execution framework consists of a master ResourceManager and a worker NodeManager on each node. Client applications submit MapReduce jobs to a JobTracker; each job splits the input data into blocks which are processed in parallel by <strong>map tasks</strong>.
                        The output of all of the map tasks is sorted by the MapReduce framework and input into <strong>reduce tasks</strong>.
                    </p>
                    <p>
                        The resouce management part is known as <strong>YARN</strong> (or Yet Another Resource Negotiator).
                        It is the component that actually manages the cluster, allowing the central NameNode to allocate resources as needed. 
                    </p>
                    <a class="btn btn-default page-scroll" href="#intro3">Tell me more!</a>
                </div>
            </div>
        </div>
    </section>

    <!-- Intro: Hadoop's MapReduce Section -->
    <section id="intro3" class="intro-section2">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h3>MapReduce - Hadoop Style</h3>
                    <p>
                        The Hadoop framework divides requests on huge, overwhelming datasets into tasks and keeps track of the data in a replicated, distributed way. Just as we saw above with MapReduce in general, Hadoop breaks input into smaller sets of data, which are processed on datanodes.
                    </p>
                    <img src="img/architecture.jpg" width="225" />
                    <p>
                        A request is known as a "job", which (since we're working with Java) includes the JAR files and classes required to process the data.
                        This job must include a main class with a datastructure that carries the configuration for the data we're working with.
                        Jobs can then be run by being submitted to Hadoop's JobTracker, which will distribute the map- and reduce-steps as tasks to the datanodes.
                        These tasks are executed in parallel by the TaskTrackers.
                        The command to start a job looks like this:
                    </p>
                    <img src="img/jobCommand.png" width="250" />
                    <br />
                    <p>
                        One really important thing to understand here is that when Hadoo's MapReduce Engine pushes work out to datanodes, it does so by keeping processing tasks as close as possible to the data which is being processed in order to reduce network traffic.
                        Why? Because there is a tradeoff between computation and communication costs!
                    </p>
                    <a class="btn btn-default page-scroll" href="#intro4">How does Hadoop manage that?</a>
                </div>
            </div>
        </div>
    </section>

    <!-- Intro: Data Locality Section -->
    <section id="intro4" class="intro-section2">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h3>A closer look at the JobTracker</h3>
                    <p>
                        The JobTracker communicates with the central NameNode to find out which node has which data.
                        Then, depending on node availability, the JobTracker assigns tasks to nodes that are as close as possible to the data.
                        A central tenant of Hadoop is that "moving computation is cheaper than moving data":
                        since we're talking <em>big data</em> here, it's better to move the task to the data, rather than moving mass amounts of data across the distributed file system. This helps avoid network congestion, which helps keep the processing time down.
                    </p>
                    <img src="img/JobTaskTracker.png" width="500"/>
                    <p>
                        Regarding failure recovery, while the NameNode <em>is</em> a single point of failure, none of the individual datanodes are. Their health is monitored through a mechanism called a <strong>heartbeat</strong>.
                        If a node does not check in with a heartbeat over a certain interval, it is presumed to have failed. The JobTracker will then assign that node's task to a different node.
                        Similarly, if a node is aware that its task has failed, it notifies the JobTracker, and depending on the type of failure, the task will either be reassigned or marked as unreliable/vulnerable.
                    </p>
                    <a class="btn btn-default page-scroll" href="#setup1">Sounds good! Ready to get set up?</a>
                </div>
            </div>
        </div>
    </section>

    <!-- Setup: EC2 Section -->
    <section id="setup1" class="setup-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h2>Setting Up</h2>
                    <p>
                        So to utilize the parallelizing gains we get from Hadoop's MapReduce, we're going to want a couple of nodes. According to the Hadoop Wiki, we're looking for "moderately high-end commodity machines" with 4-8GB of RAM. And this is where AWS EC2 comes in!
                        If you aren't familiar with AWS EC2, check out <a href="https://aws.amazon.com/ec2/">Amazon EC2 - Virtual Server Hosting</a> for an overview.
                        The main benefit of using EC2 is that we get those commodity machines we're looking for - for free!
                        There are a handful of tutorials out there for setting up a cluster through EC2. For the purposes of this how-to, written in November of 2016,
                        here is the recommended, condensed instance setup.
                    </p>
                    <img src="img/sep.png" width="300"/>
                    <h3>Step the First: Setting up EC2 Instances</h3>
                    <ol class="setup-list">
                        <li>
                            From the EC2 Management Console, select <img src="img/ec2setup1.PNG" width="100" />
                            and select <strong>Ubuntu Server 16.04 64-bit Virtual Machine</strong>. Choose <strong>t2.micro</strong>
                            for our storage (as discussed below, we aren't getting into Spark today, so our instance only needs 1 GiB in memory).
                            Select 'Configure Instance Details'.
                        </li>
                        <li>
                            Enter <strong>4</strong> for the number of instances (for our one NameNode and three DataNodes) and select 'Add Storage'.
                        </li>
                        <li>
                            Leave the root volume the default of <strong>8 GiB</strong> (for this tutorial, we won't need more than that!).
                            Select 'Add Tags' and enter a temporary name for these instances.
                            (Upon launch, we're going to change these to reflect the NameNode and DataNodes.) Select 'Configure Security Group'.
                        </li>
                        <li>
                            Create a new security group with <strong>all</strong> inbound traffic allowed
                            (NOTE: this should be <em>temporary</em> because it's extremely insecure, but it makes the cluster setup easier - it's how our NameNode will easily communicate with DataNodes).
                            Select 'Review and Launch', and then 'Launch'!
                        </li>
                        <li>
                            At this point, you should be prompted for an SSH Select an existing key pair or create a new key pair. Select <strong>Create a new key pair</strong>
                            and name it <strong>hadoop</strong>. Select 'Download Key Pair' and save this <code>hadoop.pem</code> file to a secure and find-able directory.
                            Select 'Launch Instances'.
                        </li>
                    </ol>
                    <p>
                        At this point, your instances are spinning up. This process shouldn't take more than a couple of minutes.
                        From the console, give each instance a unique tag: namenode, datanode1, datanode2, and datanode3 (these names will make our later cluster configuring easier).
                        We're now going to record a few pertinent details for setting up our multinode cluster for Hadoop.
                    </p>
                    <a class="btn btn-default page-scroll" href="#setup2">Great!</a>
                </div>
            </div>
        </div>
    </section>

    <!-- Setup: Instance Details Section -->
    <section id="setup2" class="setup-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <img src="img/sep.png" width="300" />
                    <h3>Step the Second: Getting Configuration Details from EC2 Instances</h3>
                    <ol class="setup-list">
                        <li>
                            We're going to need the <strong>Public DNS</strong> and <strong>Private DNS</strong> for each node.
                            Make a new <code>hadoopConfig.txt</code> file to keep this information around.
                        </li>
                        <li>
                            From the console, select the <strong>namenode</strong> instance.
                            In the 'Description' area, find the <strong>Public DNS</strong>, which should look something like
                            <code>ec2-35-163-47-79.us-west-2.compute.amazonaws.com</code>.
                            Copy this address into <code>hadoopConfig.txt</code>.
                        </li>
                        <li>
                            In the same 'Description' area, look for the <strong>Private DNS</strong>, which should look something like
                            <code>ip-172-31-14-44</code>. Copy this address into <code>hadoopConfig.txt</code>.
                        </li>
                        <li>
                            Repeat this process for <strong>datanode1</strong>, <strong>datanode2</strong>, and <strong>datanode3</strong>.
                            Your <code>hadoopConfig.txt</code> file should something like this (with your own instance values, of course):
                        </li>
                    </ol>
                    <img src="img/ec2setup2.PNG" width="400"/><br />
                    <a class="btn btn-default page-scroll" href="#setup3">Now that we have our instance details...</a>
                </div>
            </div>
        </div>
    </section>

    <!-- Setup: SSH-ing Into Instances -->
    <section id="setup3" class="setup-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <img src="img/sep.png" width="300" />
                    <h3>Step the Third: SSH-ing and Installation</h3>
                    <p>
                        This is where some prior knowledge of operating systems can be helpful.
                        The goal in this section is to connect to each node via SSH and to allow the <strong>NameNode</strong>
                        to SSH into each <strong>DataNode</strong> without needing to provide a password.
                        Why? This is how the <strong>JobTracker</strong> will communicate with each <strong>TaskTracker</strong>!
                    </p>
                    
                    <ol class="setup-list">
                        <li>
                            Depending on your local OS, you will either need to change the permission of the <code>hadoop.pem</code> file to 600 (for <strong>owner read-only</strong>),
                            or (if you're using a client like <a href="www.putty.org/">PuTTY</a>), you'll need to generate a private .ppk key.
                            For details, check out <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html">Connecting to Your Linux Instance from Windows Using PuTTY</a>.
                        </li>
                        <li>
                            To make it nice and simple for that namenode-to-datanode connection, we're going to create a new SSH configuration file.
                            If you're unfamiliar with this, check out <a href="https://linux.die.net/man/5/ssh_config">this</a> reference on creating a <code>~/.ssh/config</code> file.
                            Ultimately, this file should look something like this:
                        </li>
                    </ol>
                    <br /> <img src="img/config.PNG" width="500" /><br />
                    <a class="btn btn-default page-scroll" href="#setup3-2">Proceed!</a>
                </div>
            </div>
        </div>
    </section>

    <section id="setup3-2" class="intro-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h3>Finishing up that SSH bit</h3>
                    <ol class="setup-list">
                        <li>
                            Now we'll need to get the <code>hadoop.pem</code> file and this <code>config</code> file to our Linux instance.
                            Depending on your OS, you'll either need to enter <code>scp ~/.ssh/hadoop.pem ~/.ssh/config namenode:~/.ssh</code> for Linux,
                            OR install a client like <a href="https://winscp.net/eng/download.php">WinSCP</a> (which has a tidy drag-and-drop GUI)
                            to move these files over to our instance's <code>.ssh</code> directory.
                            (See the <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html">above</a> guide for more help on this.)
                        </li>
                        <li>
                            Finally, on the <strong>NameNode</strong>, we're going to create a <strong>public key</strong> to enable password-less ssh-ing to the <strong>DataNodes</strong>: </br>
                            <code>ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa</code><br />
                            <code>cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys</code><br />
                            <code>chmod 0600 ~/.ssh/authorized_keys</code><br />
                            <code>cat ~/.ssh/id_dsa.pub | ssh datanode1 'cat >> ~/.ssh/authorized_keys'</code><br />
                            <code>cat ~/.ssh/id_dsa.pub | ssh datanode2 'cat >> ~/.ssh/authorized_keys'</code><br />
                            <code>cat ~/.ssh/id_dsa.pub | ssh datanode3 'cat >> ~/.ssh/authorized_keys'</code><br />
                        </li>
                    </ol>
                    <img src="img/keyfingerprint.png" />
                    <p>
                        With that, we should be able to ssh into any <strong>DataNode</strong> from our <strong>NameNode</strong>, which means we are
                        ready to proceed with the Java and Hadoop installation and environment setup!
                    </p>
                    <a class="btn btn-default page-scroll" href="#setup4">Almost done!</a>
                </div>
            </div>
        </div>
    </section>

    <!-- Setup: Java & Hadoop Installation / Environment Config -->
    <section id="setup4" class="setup-section2">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <img src="img/sep.png" width="300" />
                    <h3>Step the Fourth: Java, Hadoop, and Environment Configuration</h3>
                    <img src="img/JobTrackTaskTracker.png" width="400" />
                    <p>
                        Now we're going to configure the virtual machines to run Hadoop!
                        The following steps should be executed on each instance (<strong>NameNode</strong>, <strong>datanode1</strong>, <strong>datanode2</strong>, and <strong>datanode3</strong>):
                    </p>
                    <ol class="setup-list">
                        <li>
                            First things first, make sure each instance is up to date:
                            <code>sudo apt-get update</code>
                        </li>
                        <li>
                            And now, since Hadoop is Java-based, we'll install Java.
                            The version will depend on the current recommended version, which we can find
                            from the official <a href="https://wiki.apache.org/hadoop/HadoopJavaVersions">Hadoop Wiki</a>.
                        </li>
                        <li>
                            And now we install the actual <a href="http://www.apache.org/dyn/closer.cgi/hadoop/common/">Hadoop files</a>...
                        </li>
                        <li>
                            With everything installed, we'll configure the environment variables.
                            See <a href="https://wiki.apache.org/hadoop/HowToConfigure">this guide</a> for more a more in-depth how to on this part,
                            paying attention to <code>hadoop-env.sh</code>, <code>slaves</code> (on the <strong>NameNode</strong>),
                            <code>hadoop-default.xml</code>, <code>mapred-default.xml</code>, and <code>hadoop-site.xml</code> files.
                        </li>
                        <li>
                            With the <code>.ssh/config</code> file and the <code>slaves</code> file configured,
                            we can start up the cluster with a single command on the <strong>NameNode</strong>: <code>$HADOOP_PREFIX/sbin/start-dfs.sh</code><br />
                            To verify that the cluster is up and running, head to <code>http://your-namenode-dns:50070</code>
                        </li>
                        <li>
                            Next, we start the resource manager on the <strong>NameNode</strong>: <code>$HADOOP_PREFIX/sbin/start-yarn.sh</code>
                        </li>
                        <li>
                            And finally, the JobHistory server! (also on the <strong>NameNode</strong>): <code>$HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR start historyserver</code>
                        </li>
                    </ol>
                    <p>
                        To check that everything is running, enter the command <code>jps</code>.
                        You should see a process list that includes <strong>TaskTracker</strong>, <strong>SecondaryNameNode</strong>, and <strong>ResourceManager</strong>.
                        Assuming everything has gone smoothly, we have completed the setup -which means we can now run our first MapReduce job!
                    </p>
                    <a class="btn btn-default page-scroll" href="#job1">It's time to MapReduce!</a>
                </div>
            </div>
        </div>
    </section>

    <!-- Job Section -->
    <section id="job1" class="job-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h2>Running a MapReduce Job</h2>
                    <img src="img/WordCountMapReduceParadigm.PNG" width="500" />
                    <p>
                        For this demo, we're going to run a standard word-count job using Hadoop's provided example job file
                        (for details on writing this job file yourself, see <a href="https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">this guide</a>).
                        Let's say we have a few books, and we would like to get a tally of words used.
                        This operation is a good candidate for the MapReduce model because we can run the tallying algorithm in parallel across the thousands of lines of text in the novels.
                        The jobfile will map each line of text into new key-value pairs of each word and its tally, then reduce by adding up all of the occurences for each word.
                        So on the <strong>NameNode</strong>, we'll do the following:
                    </p>
                    <ol class="setup-list">
                        <li>
                            First, we'll get our books into an input directory to pass along to the <strong>HDFS</strong>.
                            Check out <a href="http://www.gutenberg.org/">Project Gutenberg</a> for free ebooks, which we'll want to save in 'Plain Text UTF-8 encoding'.
                            Save a couple of books in a directory on the <strong>NameNode</strong> called<code>/MapReduceExampleInput</code>.
                            At this point, go ahead and make a <code>/MapReduceExampleOutput</code> directory for our results as well.
                        </li>
                        <li>
                            Now we'll copy these input files to the <strong>HDFS</strong>: <code>dfs -copyFromLocal /MapReduceExampleInput /user/ubuntu/MapReduceExampleInput</code>
                        </li>
                        <li>
                            And now we can run the example WordCount job! This command will run Hadoop's already-set-up wordcount jobfile,
                            using the text files we copied to the Hadoop Distributed File System, storing the output in <code>MapReduceExampleOutput</code>: <br />
                            <code>bin/hadoop jar hadoop*examples*.jar wordcount /user/ubuntu/MapReduceExampleInput /user/ubuntu/MapReduceExampleOutput</code>
                        </li>
                        <li>
                            bin/hdfs dfs -get output output
                            $ cat output/*
                            When the job finishes, we can copy the results to our local directory and view them!: <br />
                            <code>bin/hadoop dfs -get /user/ubuntu/MapReduceExampleOutput /MapReduceExampleOutput</code> <br />
                            <code>cat /MapReduceExampleOutput/*</code>
                        </li>
                    </ol>
                    <a class="btn btn-default page-scroll" href="#job2">Now, to clean up...</a>
                </div>
            </div>
        </div>
    </section>

    <!-- Job Section: Cleaning Up -->
    <section id="job2" class="job-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h2>Cleaning Up</h2>
                    <img src="img/clean.png" width="500"/>
                    <p>
                        When you're finished with a MapReduce job and don't plan on using Hadoop for a while, it's a good idea to stop the cluster
                        and <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Stop_Start.html">stop your EC2 instances</a> to avoid incurring charges:
                    </p>
                    <ol class="setup-list">
                        <li>
                            First, we'll stop the <strong>HDFS</strong> process: <br />
                            <code>$HADOOP_PREFIX/sbin/stop-dfs.sh</code>
                        </li>
                        <li>
                            Then, we'll stop the <strong>Resource Manager</strong>: <br />
                            <code>$HADOOP_PREFIX/sbin/stop-yarn.sh</code>
                        </li>
                        <li>
                            And finally, we'll stop the <strong>HistoryServer</strong>: <br />
                            <code>$HADOOP_PREFIX/sbin/mr-jobhistory-daemon.sh --config $HADOOP_CONF_DIR stop historyserver</code>
                        </li>
                        <li>
                            Enter the command <code>jps</code> to check that all processes have ended,
                            then head to your <a href="https://us-west-2.console.aws.amazon.com/ec2">AWS EC2 Dashboard</a> to stop the instances.
                        </li>
                    </ol>
                    <p>
                        We've finished the how-to portion of this guide, so to wrap up,
                        we'll briefly cover some next steps you can check out if you're curious about other MapReduce frameworks.
                    </p>
                    <a class="btn btn-default page-scroll" href="#nextsteps">Next steps</a>
                </div>
            </div>
        </div>
    </section>

    <!-- Next Steps Section -->
    <section id="nextsteps" class="job-section2">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h2>What's next?</h2>
                    <img src="img/Spark.JPG" width="500"/>
                    <p>
                        If you are intrigued by MapReduce and are curious to learn what's up-and-coming in the big data analytics world,
                        consider checking out <a href="http://spark.apache.org/">Apache Spark</a>.
                        Similar to Hadoop, Spark is a newer, alternative big data processing engine.
                        By using a <strong>DAG (Directed Acyclic Graph)</strong> engine, Spark uses in-memory caching, which makes it <em>much</em> faster than Hadoop's MapReduce engine.
                        Programs can easily be written in higher-level languages like Python.
                        Furthermore, it makes iterative tasks (like machine learning!) feasible.
                        Its main drawbacks are that it is memory-intensive, and for some tasks, you may have to manually manage some of the things Hadoop MapReduce takes care of for you.
                    </p>
                    <p>
                        AWS offers an <a href="https://aws.amazon.com/emr">Elastic MapReduce</a>, which makes life easy by providing and configuring Apache Hadoop and Spark software.
                        For a great getting-started tutorial (complete with a Department of Transportation example), see <a href="https://aws.amazon.com/blogs/aws/new-apache-spark-on-amazon-emr/">Apache Spark on Amazon EMR</a>.
                        When you've got an environment up and running, you can check out <a href="http://spark.apache.org/examples.html">more examples</a>.
                    </p>
                    <p>
                        And thus concludes our MapReduce and Hadoop overview! See below for a list of resources used to write this guide.
                    </p>
                    <a class="btn btn-default page-scroll" href="#resources">Resources</a>
                </div>
            </div>
        </div>
    </section>

    <!-- Resources Section -->
    <section id="resources" class="resources-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h1>Resources</h1>
                    <ol class="resources-list">
                        <li><a href="http://research.google.com/archive/mapreduce.html">MapReduce - Google Research Publication</a></li>
                        <li><a href="https://en.wikipedia.org/wiki/MapReduce">MapReduce - Wikipedia</a></li>
                        <li><a href="http://deltarho.org/docs-datadr/#mapreduce">Introduction to MapReduce</a></li>
                        <li><a href="https://wiki.apache.org/hadoop">Apache Hadoop Wiki</a></li>
                        <li><a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html">Hadoop Cluster Setup</a></li>
                        <li><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster">Hadoop Single Node Cluster Setup on Ubuntu</a></li>
                        <li><a href="http://spark.apache.org/">Apache Spark</a></li>
                    </ol>
                </div>
            </div>
        </div>
    </section>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Scrolling Nav JavaScript -->
    <script src="js/jquery.easing.min.js"></script>
    <script src="js/scrolling-nav.js"></script>

</body>

</html>